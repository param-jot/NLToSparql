{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NamedEntityRec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/param-jot/NLToSparql/blob/main/NamedEntityRec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsTBDjCsyUfm"
      },
      "source": [
        "!pip install -q -U trax\n",
        "import trax\n",
        "# Install JAX.\n",
        "!pip install --upgrade jax\n",
        "!pip install --upgrade jaxlib\n",
        "!pip install --upgrade trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJYaeeOI359I"
      },
      "source": [
        "from trax import layers as tl\n",
        "import os # For os dependent functionalities\n",
        "import numpy as np # For scientific computing\n",
        "import pandas as pd # For basic data analysis\n",
        "import random as rnd # For using random functions\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzOy7eqlbbm3"
      },
      "source": [
        "data = pd.read_csv(\"dataset.csv\",encoding = 'ISO-8859-1')\n",
        "data = data.fillna(method = 'ffill')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJURF7323qf2"
      },
      "source": [
        "## Extract the 'Word' column from the dataframe\n",
        "words = data.loc[:, \"Word\"]\n",
        "\n",
        "## Convert into a text file using the .savetxt() function\n",
        "np.savetxt(r'words.txt', words.values, fmt=\"%s\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9tHCP09eRMZ"
      },
      "source": [
        "import json\n",
        "vocab = {}\n",
        "with open('words.txt') as f:\n",
        "  for i, l in enumerate(f.read().splitlines()):\n",
        "    vocab[l] = i\n",
        "  print(\"Number of words:\", len(vocab))\n",
        "  vocab['<PAD>'] = len(vocab)\n",
        "  print(vocab)\n",
        "  #file = open('vocab.subword', mode='w')\n",
        "  json.dump(vocab, file)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9manNO4gxJQ"
      },
      "source": [
        "class Get_sentence(object):\n",
        "    def __init__(self,data):\n",
        "        self.n_sent=1\n",
        "        self.data = data\n",
        "        agg_func = lambda s:[(w,p,t) for w,p,t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                     s[\"POS\"].values.tolist(),\n",
        "                                                     s[\"Type\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "      "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHMx6ZeF2WtU"
      },
      "source": [
        "getter = Get_sentence(data)\n",
        "sentence = getter.sentences"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byQM4JtsPdfy"
      },
      "source": [
        "words = list(set(data[\"Word\"].values))\n",
        "words_tag = list(set(data[\"Type\"].values))\n",
        "\n",
        "word_idx = {w : i+1 for i ,w in enumerate(words)}\n",
        "tag_idx =  {t : i for i ,t in enumerate(words_tag)}\n",
        "print(words)\n",
        "print(word_idx)\n",
        "print(words_tag)\n",
        "print(tag_idx)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTFPj4mMPI2A"
      },
      "source": [
        "X = [[word_idx[w[0]] for w in s] for s in sentence]\n",
        "y = [[tag_idx[w[2]] for w in s] for s in sentence]\n",
        "print(len(sentence))\n",
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zPQa1biUmyh"
      },
      "source": [
        "def data_generator(batch_size, x, y,pad, shuffle=False, verbose=False):\n",
        "\n",
        "    num_lines = len(x)\n",
        "    lines_index = [*range(num_lines)]\n",
        "    if shuffle:\n",
        "        rnd.shuffle(lines_index)\n",
        "    \n",
        "    index = 0 \n",
        "    while True:\n",
        "        buffer_x = [0] * batch_size \n",
        "        buffer_y = [0] * batch_size \n",
        "        print(batch_size)\n",
        "        max_len = 0\n",
        "        for i in range(batch_size):\n",
        "            if index >= num_lines:\n",
        "                index = 0\n",
        "                if shuffle:\n",
        "                    rnd.shuffle(lines_index)\n",
        "            \n",
        "            buffer_x[i] = x[lines_index[index]]\n",
        "            buffer_y[i] = y[lines_index[index]]\n",
        "            \n",
        "            lenx = len(x[lines_index[index]])    \n",
        "            if lenx > max_len:\n",
        "                max_len = lenx                  \n",
        "            \n",
        "            index += 1\n",
        "\n",
        "        X = np.full((batch_size, max_len), pad)\n",
        "        print(batch_size)\n",
        "        Y = np.full((batch_size, max_len), pad)\n",
        "        print(batch_size)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            x_i = buffer_x[i]\n",
        "            y_i = buffer_y[i]\n",
        "            for j in range(len(x_i)):\n",
        "\n",
        "                X[i, j] = x_i[j]\n",
        "                Y[i, j] = y_i[j]\n",
        "\n",
        "        if verbose: print(\"index=\", index)\n",
        "        yield((X,Y))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kabGkjJiFPxd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,random_state=1)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_QT1sgNCKbC"
      },
      "source": [
        "\n",
        "def NERmodel(tags, vocab_size=23400, d_model = 50):\n",
        "\n",
        "  model = tl.Serial(\n",
        "    # tl.Embedding(vocab_size, d_model),\n",
        "    trax.models.reformer.Reformer(vocab_size, d_model, ff_activation=tl.LogSoftmax),\n",
        "    #change vocab to input vocab\n",
        "    tl.Dense(tags),\n",
        "    tl.LogSoftmax()\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t772b7MCNXq"
      },
      "source": [
        "model = NERmodel(tags = 17)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDAkCnRCCPnb"
      },
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "rnd.seed(33)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_generator = trax.data.inputs.add_loss_weights(\n",
        "    data_generator(batch_size, x_train, y_train,vocab['<PAD>'], True),\n",
        "    id_to_mask=vocab['<PAD>'])\n",
        "print(x_train)\n",
        "print(y_train)\n",
        "print(batch_size)\n",
        "\n",
        "eval_generator = trax.data.inputs.add_loss_weights(\n",
        "    data_generator(batch_size, x_test, y_test,vocab['<PAD>'] ,True),\n",
        "    id_to_mask=vocab['<PAD>'])\n",
        "print(vocab)\n",
        "print(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV9wMogACVBi"
      },
      "source": [
        "def train_model(model, train_generator, eval_generator, train_steps=1, output_dir='model'):\n",
        "    train_task = training.TrainTask(\n",
        "      train_generator,  \n",
        "      loss_layer = tl.CrossEntropyLoss(), \n",
        "      optimizer = trax.optimizers.Adam(0.01), \n",
        "      n_steps_per_checkpoint=1\n",
        "    )\n",
        "\n",
        "    eval_task = training.EvalTask(\n",
        "      labeled_data = eval_generator, \n",
        "      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], \n",
        "      n_eval_batches = 1\n",
        "    )\n",
        "\n",
        "    training_loop = training.Loop(\n",
        "        model, \n",
        "        train_task, \n",
        "        eval_tasks = eval_task, \n",
        "        output_dir = output_dir) \n",
        "\n",
        "    training_loop.run(n_steps = train_steps)\n",
        "    return training_loop"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ovWFZcCYHq"
      },
      "source": [
        "train_steps = 1\n",
        "training_loop = train_model(model, train_generator, eval_generator, train_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OcLbswYQ_E3"
      },
      "source": [
        "Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztCtopRccBSM"
      },
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "#Loading\n",
        "#gin.parse_config('/content/model/config.gin')\n",
        "\n",
        "model = trax.models.reformer.Reformer(\n",
        "    input_vocab_size=33300,\n",
        "    d_model=512, d_ff=2048,\n",
        "    n_heads=8, n_encoder_layers=6, n_decoder_layers=6,\n",
        "    max_len=2048, mode='eval')\n",
        "\n",
        "#Initialization\n",
        "\n",
        "model.init_from_file('/content/model/model.pkl.gz',\n",
        "                     weights_only=True)\n",
        "\n",
        "#sample sentence\n",
        "sentence = 'German call to boycott British lamb'\n",
        "print(sentence)\n",
        "#Encoding of a sentence\n",
        "tokens = [word_idx[w] for w in sentence.split(\" \")]      \n",
        "print(type(tokens))\n",
        "print(tokens) \n",
        "\n",
        "array_token = np.array(tokens)\n",
        "print(type(array_token))\n",
        "\n",
        "#array_tokens = np.array_split(tokens,3)\n",
        "print(array_token)\n",
        "\n",
        "# Decode from the Reformer\n",
        "tokenized = array_token[None, :]  # Add batch dimension.\n",
        "print(\"shape of an array: \", tokenized.shape)\n",
        "print(tf.shape(tokenized))\n",
        "print(type(tokenized))\n",
        "print(tokenized)\n",
        "\n",
        "tokenized_translation = trax.supervised.decoding.autoregressive_sample(\n",
        "    model, tokenized, temperature=0.0)\n",
        "print(tokenized_translation)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcVuT2Upp5PE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}